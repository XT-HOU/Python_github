什么是Spark
    Spark是一种基于内存的快速、通用、可扩展的大数据分析计算引擎(框架)

Spark内置模块
    1.Spark Core：实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块。
Spark Core中还包含了对弹性分布式数据集(Resilient Distributed DataSet，简称RDD)的API定义。
    2.Spark SQL：是Spark用来操作结构化数据的程序包。通过Spark SQL，我们可以使用 SQL或者Apache Hive版本的HQL来查询数据。
Spark SQL支持多种数据源，比如Hive表、Parquet以及JSON等。
    3.Spark Streaming：是Spark提供的对实时数据进行流式计算的组件。提供了用来操作数据流的API，
并且与Spark Core中的 RDD API高度对应。
    4.Spark MLlib：提供常见的机器学习功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。
    5.Spark GraphX：主要用于图形并行计算和图挖掘系统的组件。
    6.集群管理器：Spark设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算。为了实现这样的要求，同时获得最大灵活性，
Spark支持在各种集群管理器(Cluster Manager)上运行，包括Hadoop YARN、Apache Mesos，
以及Spark自带的一个简易调度器，叫作独立调度器。

Spark特点
    1)快：与hadoop的MapReduce相比，spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，
可以通过基于内存来高效处理数据流。计算的中间结果是存在于内存中的。
    2)易用：Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用用户可以快速构建不同的应用。。。。
    3)通用：
    4)兼容：

Spark运行模式
    Loacal模式：在本地部署单个Spark服务。
        spark-submit --master local[2] pi.py
    Standalone模式：Spark自带的任务调度模式。
        spark-submit --master spark://hadoop01:7077 pi.py
        --deploy-mode client，表示Driver程序运行在本地客户端，Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出
        --deploy-mode cluster，表示Driver程序运行在集群，适用于生产环境
    YRAN模式：Spark使用Hadoop的YARN组件进行资源与任务调度。
        --deploy-mode client，表示Driver程序运行在本地客户端，Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出
        --deploy-mode cluster，表示Driver程序运行在集群，Driver程序运行在由ResourceManager启动的APPMaster适用于生产环境
Spark安装
    1）官网地址：http://spark.apache.org/
    2）文档查看地址：https://spark.apache.org/docs/2.4.5/
    3）下载地址：https://spark.apache.org/downloads.html


Driver：算子以外的代码都是在Driver端执行
Executor：算子里面的代码都是在Executor端执行
